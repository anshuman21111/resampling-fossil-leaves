---
title: "Sampling bias"
author: "Anshuman"
date: "1/18/2021"
output: html_document
---


```{r}
#Listing and importing files 
filenets = list.files(path="data/Localities/select", pattern="*.csv", full.names=T)
filenets2 = list.files(path="data/Localities/select", pattern="*.csv", full.names=F) #obtaining site name from file name
lengthi=NULL
for (i in 1:length(filenets)){
  nam=paste(filenets[i])
  netcurr=read.csv(nam, header=T)
  colnames(netcurr)[1]="ID"
  nam2=strsplit(filenets2[i],".csv")[[1]]
  bstill=c(nam2,nrow(netcurr))
  lengthi=rbind(lengthi,bstill)
}
#calculating sample size
rownames(lengthi)=NULL
leng2=lengthi
colnames(leng2)=c("web", "samples")
leng2=as.data.frame(leng2)
leng2$samples=as.numeric(leng2$samples)
leng3=leng2[order(leng2$samples),]
```


```{r}
#setting sample size categorizations
allseq=c(seq(100,500,50),seq(600,1000,100),seq(1200,2000,200), seq(2500,4000,500), seq(5000,7500,1000))

custlen=function(x){
  return(length(allseq[allseq<x]))
}

newcustlen=function(x){
  if (x<500){return("A")}
  if (x>=500 & x<1000){return("B")}
  if (x>=1000 & x<2000){return("C")}
  if (x>=2000 & x<4000){return("D")}
  if (x>=4000 & x<9000){return("E")}
}

```

```{r}
grp=NULL
for (i in 1:61){
  grp=c(grp,newcustlen(leng2$samples[i]))
}
```



```{r}
#calculating network level and node level network metrics for each of the networks while accounting for resampling
set.seed(1)

networklevlist=NULL
networklevmean=NULL
networklevelvar=NULL

library(Rfast)

for (i in 1:length(filenets)){
  nam=paste(filenets[i])
  netcurr=read.csv(nam, header=T)
  colnames(netcurr)[1]="ID"
  netcurr2=netcurr %>% group_by(ID) %>%  summarise_if(is.numeric, sum, na.rm = TRUE)
  netmat = data.matrix(netcurr2[,-c(1)], rownames.force = NA)
  rownames(netmat)= netcurr2$ID
  
  minrow=300
  
  netpro=NULL
  allnames=cbind(rownames(netmat),0)
  colnames(allnames)=c("ID","Nul")
  allnames=as.data.frame(allnames)
  
  for (j in 1:500){
        q2=sample(nrow(netcurr), minrow, replace=F)
        datarand=netcurr[q2,]
        netrand=datarand %>% group_by(ID) %>%  summarise_if(is.numeric, sum, na.rm = TRUE)
        netrandfinal=left_join(allnames,netrand, by="ID")
        netrandfinal[is.na(netrandfinal)] <- 0
        netrandfinal=netrandfinal[,-c(1:2)]
        netrandfinal=data.matrix(netrandfinal)
        netpro=rbind(netpro,networklevel(netrandfinal, nrep=20))
        
        if (j %% 10 == 0){print(paste("Sequence", " ", j))}
  }
  
  
  
  nam2=strsplit(filenets2[i],".csv")[[1]]
  
  networklevmean=rbind(networklevmean,c(nam2,colmeans(netpro)))
  networklevelvar=rbind(networklevelvar,c(nam2,colVars(netpro, std = F)))
  
  networklevlist[[i]]=netpro
 
  print(i)
}

colnames(networklevmean)=c("Web",colnames(netpro))
colnames(networklevelvar)=c("Web",colnames(netpro))
```


```{r}
#repeating the same with categorizations
set.seed(1)

networklevlist=NULL
#networklevmean=NULL
#networklevelvar=NULL

library(Rfast)
#length(filenets)
for (i in 1:2){
  netpro=NULL
  nam=paste(filenets[i])
  netcurr=read.csv(nam, header=T)
  colnames(netcurr)[1]="ID"
  netcurr2=netcurr %>% group_by(ID) %>%  summarise_if(is.numeric, sum, na.rm = TRUE)
  netmat = data.matrix(netcurr2[,-c(1)], rownames.force = NA)
  rownames(netmat)= netcurr2$ID
  
 
  
  seqsp=NULL
  allnames=cbind(rownames(netmat),0)
  colnames(allnames)=c("ID","Nul")
  allnames=as.data.frame(allnames)
  #custlen(leng2$samples[i])
  
  
  for (num in 1:2){
    minrow=allseq[num]
  
    for (j in 1:50){
        q2=sample(nrow(netcurr), minrow, replace=F)
        datarand=netcurr[q2,]
        netrand=datarand %>% group_by(ID) %>%  summarise_if(is.numeric, sum, na.rm = TRUE)
        netrandfinal=left_join(allnames,netrand, by="ID")
        netrandfinal[is.na(netrandfinal)] <- 0
        netrandfinal=netrandfinal[,-c(1:2)]
        netrandfinal=data.matrix(netrandfinal)
        netpro=rbind(netpro,networklevel(netrandfinal, nrep=20))
        
        
        if (j %% 25 == 0){print(paste("Web",i,":",strsplit(filenets2[i],".csv")[[1]], "Sample size", allseq[num], "Sequence", j))}
    }
     
    seqsp=c(seqsp, rep(allseq[num], 50))
    
  }
  
  nam2=strsplit(filenets2[i],".csv")[[1]]
  
  netpro=as.data.frame(netpro)
  
  netpro$web=nam2
  
  netpro$sample.size= seqsp
  
  
  networklevlist[[i]]=netpro
 
  print(i)
}
  


#colnames(networklevmean)=c("Web",colnames(netpro))
#colnames(networklevelvar)=c("Web",colnames(netpro))
```



```{r}
#exporting files
#write.csv(networklev, "networklevelAll.csv")
#write.csv(higherlev,"DTlevelAll.csv")
#write.csv(plantlev, "plantlevelAll.csv")
```


```{r}
#concat
mat <- matrix(1:25, ncol = 5)
vec <- seq(2, by = 2, length = 5)

sweep(mat, 2, vec, `/`)
```




```{r}
#whole dataset values
filenets = list.files(path="data/Localities/select", pattern="*.csv", full.names=T)
filenets2 = list.files(path="data/Localities/select", pattern="*.csv", full.names=F)

networklevn=NULL

for (i in 1:length(filenets)){
  nam=paste(filenets[i])
  netcurr=read.csv(nam, header=T)
  colnames(netcurr)[1]="ID"
  netcurr2=netcurr %>% group_by(ID) %>%  summarise_if(is.numeric, sum, na.rm = TRUE)
  netmat = data.matrix(netcurr2[,-c(1)], rownames.force = NA)
  rownames(netmat)= netcurr2$ID
  
  
  

  nam2=strsplit(filenets2[i],".csv")[[1]]
  #plotweb(net)
  networklevn=rbind(networklevn,c(nam2,networklevel(netmat)))
  print(i)
}


```



```{r}
networklevn2=networklevn[,-c(1,6)]

netwoo2=NULL
netwoodata=NULL

for (i in 1:61){
  X=data.matrix(as.data.frame(netwoo[[i]][,-c(5,48,49)]))
  X2=sweep(X, 2, as.numeric(networklevn2[i,]), `/`)
  netwoo2[[i]]=X2
  X3=as.data.frame(cbind(as.data.frame(netwoo[[i]][,c(48,49)]),X2))
  netwoodata=rbind(netwoodata,X3)
}


netwoodata2=netwoodata

tab=table(netwoodata$web)

grp2=NULL
for (i in 1:61){
  grp2=c(grp2,rep(grp[i],as.numeric(tab[i])))
}

netwoodata2$group=as.factor(grp2)
```


```{r}
#finding deviations
 norm.interval1 = function(data, variance = var(data), conf.level = 0.95) {
 z = qnorm((1 - conf.level)/2, lower.tail = FALSE)
 xbar = mean(data)
 sdx = sqrt(variance/length(data))
 c(xbar - z * sdx)
 }

 norm.interval2 = function(data, variance = var(data), conf.level = 0.95) {
 z = qnorm((1 - conf.level)/2, lower.tail = FALSE)
 xbar = mean(data)
 sdx = sqrt(variance/length(data))
 c(xbar + z * sdx)
 }

norm.interval1(netwoodata2$C.score.LL)
mean(netwoodata2$C.score.LL)

summary(netwoodata2$C.score.LL)
```



```{r}
hist(leng2$samples, main="Distribution",xlim = c(0,7500), breaks=c(0,500,1000,2000,4000,7500), col = c("blue", "red","green", "grey"))

h=hist(leng2$sample, breaks=35, col="indianred2", xlim = c(0,7500), main = "" )
cuts <- cut(h$breaks, c(0,500,1000,2000,4000,7500))
plot(h, col=c(1:5)[cuts])
```







